<!DOCTYPE html>
<html>
<head></head>
<body>

<button id="audio_to_timeseries" onclick="audio_to_timeseries()" style="display:block;">audio_to_timeseries</button>
	
<button id="audio_to_frequency_domain" onclick="audio_to_frequency_domain()" style="display:block;">audio_to_frequency_domain</button>

<button id="audio_to_spectrogram" onclick="audio_to_spectrogram()" style="display:block;">audio_to_spectrogram</button>
  
<div id="data_display" style="display:block; text-align: left; width: 600px; height: 600px">


 
<!-- --------------------------------------------------- -->

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.7.1/jquery.min.js" integrity="sha512-v2CJ7UaYy4JwqLDIrZUI/4hqeoQieOmAZNXBeQyjo21dadnwR+8ZaIJVT8EE2iyI61OV8e6M8PP2/4hpQINQ/g==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>

<script src='https://d3js.org/d3.v7.min.js'></script>

<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest"></script>

<script src="https://cdn.plot.ly/plotly-2.30.0.min.js" charset="utf-8"></script>

<script>

	
// var url = "https://interactive-examples.mdn.mozilla.net/media/cc0-audio/t-rex-roar.mp3";

// -------------------------------------------------
	
var audio_information = {};

async function audio_to_timeseries() {
	
	var url = "https://interactive-examples.mdn.mozilla.net/media/cc0-audio/t-rex-roar.mp3";

	// Create audioElement
	var audioElement = await load_audio_from_url(url, mode="nocors");
  
	// -----------------------------------------------

	// Obtain audio information
	audio_information = await evaluate_audioElement();
	console.log('audio_information: ', audio_information);
	// {channels: channels, fs: fs, time_length: time_length, frameCount: frameCount, time_sample_rate_OR_timePeriod: time_sample_rate_OR_timePeriod}
	
	// -----------------------------------------------
  
	// Fetch binaryString of audio data
	var settings = {
		type : 'GET',
		async: true,
		crossDomain: true,
		xhrFields: {responseType: 'arrayBuffer'},
		dataType: 'binary',
		beforeSend: function(xhr) {xhr.withCredentials = true;},
		success: function(response) { console.log('Success'); },
		error: function(xhr, status, error) { console.log('Error:', error); }
	};
 
	var binaryString = await $.ajax(url, [,settings]).done(function(response) { return response; });
	console.log('binaryString: ', binaryString);

	// -----------------------------------------------

	// Normalize the audio data
	const normalArray_normalized = await decodeAudioData_from_binaryString_to_uint8Array(binaryString);
	const t0 = Array.from({ length: normalArray_normalized.length }, (_, i) => i * audio_information.time_sample_rate_OR_timePeriod);
	
	// await plot_line_graph_dataObject(t0, normalArray_normalized, "Timeseries: both channels");
	
	// --------------------

	// Separate the channels of the audio data
	const [channel1, channel2] = await separate_stero_channels(normalArray_normalized)

	console.log('channel1.length : ', channel1.length);
	console.log('channel2.length : ', channel2.length);

	const t1 = Array.from({ length: normalArray_normalized.length }, (_, i) => i * audio_information.time_sample_rate_OR_timePeriod);

	// await plot_line_graph_dataObject(t1, channel1, "Timeseries: channel 1");
	// await plot_line_graph_dataObject(t1, channel2, "Timeseries: channel 2");

	// --------------------

	// Average the audio data channels
	const x_avg = await channel1.map((val, ind) => { return (val + channel2.at(ind))/2; });
	console.log('x_avg.length : ', x_avg.length);

	// await plot_line_graph_dataObject(t1, x_avg, "Timeseries: channels averaged");
	await plotly_line_chart(t1, x_avg, "Timeseries: channels averaged", "Time (s)", "Audio amplitude");

	return [t1, x_avg];
}

  
// -------------------------------------------------


async function audio_to_frequency_domain() {
	
	const [t1, x_avg] = await audio_to_timeseries();
	
	// --------------------
  
	// Compute frequency response/domain
	var freq_response = await tensorflow_FFT(x_avg);
	console.log('freq_response : ', freq_response);
	
	// --------------------

	// Magnitude of frequency response
	const Xn_mag = tf.real(freq_response).abs();

	// Convert to JavaScript
	Xn_mag = Xn_mag.arraySync();

	// Phase of frequency response
	const Xn_phase = tf.imag(freq_response);

	// Convert to JavaScript
	Xn_phase = Xn_phase.arraySync();

	console.log('Xn_mag.length : ', Xn_mag.length);
  
	// Calculate frequency: start from zero
	const f = Array.from({ length: Xn_mag.length }, (_, i) => (i-1) * 2 * Math.PI * 1/audio_information.time_sample_rate_OR_timePeriod);
	// console.log('f.length : ', f.length);

	const N = f.length;
	
	// --------------------

	// (Optional) Put the frequency response in the correct order : fft gives the result on a circle but mapped to a line it give the negative part first and then the positive part.

	// Magnitude
	const negative_part_mag = Xn_mag.slice(Math.floor(N/2), N);
	const positive_part_mag = Xn_mag.slice(0, Math.floor(N/2));
	const Xn_mag_correct_order = negative_part_mag.concat(positive_part_mag);
	console.log("Xn_mag_correct_order.length: ", Xn_mag_correct_order.length);

	// Phase
	const negative_part_phase = Xn_phase.slice(Math.floor(N/2), N);
	const positive_part_phase = Xn_phase.slice(0, Math.floor(N/2));
	const Xn_phase_correct_order = negative_part_phase.concat(positive_part_phase);
	console.log("Xn_phase_correct_order.length: ", Xn_phase_correct_order.length);

	// frequency
	const positive_part_f = f.slice(0, Math.floor(N/2));
	const negative_part_f = positive_part_f.reverse();

	// remove last value, the zero
	negative_part_f.pop();
	const f_correct_order = negative_part_f.concat(positive_part_f);

	// Add one value on the end to make f the same length as magnitude and phase
	f_correct_order = f_correct_order.concat((2 * Math.PI * N/2)/N); // it reduces to PI, but for clarity of how it is obtained the entire equation is written
	console.log("f_correct_order.length: ", f_correct_order.length);

	// Plot both the [negative, positive] parts of the frequency response in hertz
	const title_text = "Frequency response in correct order: Hz";
	const x_text_trace1 = "Frequency";
	const y_text_trace1 = "Magnitude (Hz)";
	const x_text_trace2 = "Frequency";
	const y_text_trace2 = "Phase (deg)";
	var trace1 = {x: f_correct_order, y: Xn_mag_correct_order, xaxis: x_text_trace1, yaxis: y_text_trace1, mode: 'markers', type: 'line'};
	var trace2 = {x: f_correct_order, y: Xn_phase_correct_order, xaxis: x_text_trace2, yaxis: y_text_trace2, mode: 'markers', type: 'line'};
	var data = [trace1, trace2];
	var layout = {grid: {rows: 2, columns: 1, pattern: 'independent'}, title: title_text};
	Plotly.newPlot('data_display', data, layout);
  
	// --------------------
  
	// Put the frequency response in the correct order : fft gives the result on a circle but mapped to a line it gives the negative part first and then the positive part. Only select the positive part.
	const Xn_mag_positive_part_db = positive_part_mag.map((val, ind) => { return 20 * Math.log10(val); });

	// Plot only positive part of frequency response in decibels
	title_text = "Frequency response in decibels: dB";
	x_text_trace1 = "Frequency";
	y_text_trace1 = "Magnitude (dB)";
	x_text_trace2 = "Frequency";
	y_text_trace2 = "Phase (deg)";
	trace1 = {x: positive_part_f, y: Xn_mag_positive_part_db, xaxis: x_text_trace1, yaxis: y_text_trace1, mode: 'markers', type: 'line'};
	trace2 = {x: positive_part_f, y: positive_part_phase, xaxis: x_text_trace2, yaxis: y_text_trace2, mode: 'markers', type: 'line'};
	data = [trace1, trace2];
	layout = {grid: {rows: 2, columns: 1, pattern: 'independent'}, title: title_text};
	Plotly.newPlot('data_display', data, layout);
  
	// --------------------
	
}

	
// -----------------------------------------------

	
async function audio_to_spectrogram() {
	
	const [t1, x_avg] = await audio_to_timeseries();
	
	// --------------------
	
	// Calculate spectrogram
	var frameLength = 255;  // fftSize or nfft, Length of each window/frame segment - frequency data will be binned by nfft/2 
	var frameStep = 128;  // The number of samples to shift between frames.
	var fftLength = x_avg.length;  // The length of the FFT used for the STFT.
	var windowFn = tf.signal.hann_window; // The window function to apply to each frame before computing the FFT; to smooth the data


	// Spectrogram is x (time) by y (frequency), so chunk the time series and then compute the FFT per [time series chunk]
	
	// Try to make up a reasonable time chuck for the length of time series data
	const num_of_timeBins = Math.floor(x_avg.length/10); // make each time series chunk a 10% of the data length

	var spec_width_height = [];

	// For plotting
	var plot_width_time = [];
	var max_freq = 0;
	
	for (var width=0; width<num_of_timeBins-1; width++) {
		var start = width * num_of_timeBins;
		var x_time_chunk = x_avg.slice(start, start+num_of_timeBins);
		console.log('x_time_chunk : ', x_time_chunk);

		plot_width_time.push(width);
		
		// Convert the [waveform chunk] to a spectrogram using the Short-Time Fourier Transform (STFT)
	  	var spectrogram_height = tf.signal.stft(x_time_chunk, frameLength, frameStep, fftLength, windowFn);
		// console.log('spectrogram_height : ', spectrogram_height);
		// dtype: "complex64"
		// shape: Array [ 0, 129 ] which corresponds to [batch_size, height=frequency]
		// strides: Array [ 129 ]
	
		// Obtain the magnitude of the STFT (dtype: "float32")
		spectrogram_height = tf.abs(spectrogram_height);
		// console.log('spectrogram : ', spectrogram);
		// dtype: "float32"
		// shape: Array [0, 129]
		// strides: Array [ 129 ]

		// Convert to JavaScript array, to pack into a JavaScript array
		const jsArray = spectrogram_height.arraySync();
		console.log('jsArray: ', jsArray);

		// Keep track of maximum value of frequency, to make frequency plot axis
		if (Math.max(jsArray) > max_freq) {
			max_freq = cur_max_fres;
		}

		spec_width_height.push(jsArray);
	}


	// spec_width_height should be 2D (width=time, height=frequency), so convert to 2Dtensor 
	const spec_width_height2d = tf.tensor2d(spec_width_height);
	
  	// Transpose 
	const spec_height_width2d = tf.transpose(spec_width_height2d);
	
	// Add an extra-dimension for the batch_size 
	const spec_batch_height_width3d = spec_height_width2d.expandDims(0);

	// Add an extra-dimension for the channel
	// Add a `channels` dimension, so that the spectrogram can be used as image-like input data with convolution layers (which expect
	const spec_batch_height_width_channels4d = spec_batch_height_width3d.expandDims(-1);
	console.log('spec_batch_height_width_channels4d : ', spec_batch_height_width_channels4d);
	

	// Desired Output: 4D Array
	// shape (`batch_size`, `height`, `width`, `channels`)


	const spectrogramArray = spec_batch_height_width_channels4d.arraySync();

	// --------------------

	// Plot 3D data on canvas
	var w = frameLength;
	const ctx = await create_dynamic_canvasElement(w);
	
	// Manipulate image data, change the alpha component
	// Way 0: use the 4D data array
	let imageDataArray = new Uint8ClampedArray(w * w * 4);
	for (let i = 0; i < w * w; i++) {
	     let value = spectrogramArray.dataSync()[i];
	     imageDataArray[i * 4] = value > 0.5 ? 255 : 0; // Red component
	     imageDataArray[i * 4 + 1] = value > 0.5 ? 255 : 0; // Green component
	     imageDataArray[i * 4 + 2] = value > 0.5 ? 255 : 0; // Blue component
	     imageDataArray[i * 4 + 3] = 255; // Alpha component, set to 255 to make it fully opaque
	}

	// Print the manipulated image data on another canvas
	let imageData = new ImageData(imageDataArray, w, w);
	
	// Draw the image data onto the canvas
	ctx.putImageData(imageData, 0, 0);

	// --------------------
	
	// OR

	// Convert to JavaScript array, to pack into a JavaScript array
	const jsArray_2d = spec_height_width2d.arraySync();
	const y_freq = Array.from({ length: jsArray_2d.length }, (_, i) => (i-1) * max_freq/jsArray_2d.length);
	
	var data = [
	  {
	    z: jsArray_2d,
	    x: plot_width_time,
	    y: y_freq,
	    type: 'heatmap',
	    hoverongaps: false
	  }
	];
	
	Plotly.newPlot('data_display', data);

}
  
// -----------------------------------------------













async function plotly_line_chart(x, y, title_text, x_text, y_text) {
        // https://plotly.com/javascript/getting-started/
	const xValue = [...Array(x.length).keys()].map((x) => x);
	var yValue = y;
	
        const trace_items = { x: xValue, y: yValue, mode: 'markers', type: 'line' };
        const layout = {title: title_text, xaxis: {title: x_text}, yaxis: {title: y_text} };
        Plotly.newPlot("data_display", [trace_items], layout );
}

// -----------------------------------------------
	
async function plot_line_graph_dataObject(x, y, title_text) {

	// const dataObject = [
        //     { x: 0, y: 10 },
        //     { x: 1, y: 20 },
        //     { x: 2, y: 15 },
        //     { x: 3, y: 25 },
        //     { x: 4, y: 18 }
        // ];

	console.log("x: ", x.slice(0, 10));
	console.log("y: ", y.slice(0, 10));
	
	var dataObject = [];
	for (var i=0; i<x.length; i++) {
		var obj = { x: x.at(i), y: y.at(i) };
		dataObject.push(obj);
	}
	
	const width = 1000;
	const height = 500;

	const margin = {top: 20, right: 30, bottom: 30, left: 40};
	
        const svg = d3.select("#data_display")
		.append("svg")
		.attr("class", 'line')
		.attr("width", width)
		.attr("height", height);

        const x_scale = d3.scaleLinear()
            .domain([0, d3.max(dataObject, d => d.x)], [margin.left, width - margin.right])
            .range([0, width]);

        const y_scale = d3.scaleLinear()
            .domain([0, d3.max(dataObject, d => d.y)], [height - margin.bottom, margin.top])
            .range([height, 0]);

        const line = d3.line()
            .x(d => x_scale(d.x))
            .y(d => y_scale(d.y));

        svg.append("path")
            .datum(dataObject)
            .attr("fill", "none")
            .attr("stroke", "steelblue")
            .attr("stroke-width", 2)
            .attr("d", line);

	// --------------------
	
	// Add the x-axis
	svg.append("g")
		.attr('class', 'x axis')
		// The next line moves the axis to the bottom
		.attr("transform", `translate(0, ${height - margin.bottom})`)
		.call(d3.axisBottom(x_scale));

	// --------------------
	
	// Add the y-axis
	svg.append("g")
		.attr('class', 'y-axis')
		.attr("transform", `translate(${margin.left}, 0)`)
		.call(d3.axisLeft(y_scale))
		// Add title
		.call(g => g.append("text")
			.attr("x", d3.max(dataObject, d => d.x)/2 )
			.attr("y", 15)
			.attr("fill", "currentColor")
			.attr("text-anchor", "middle")
			.style("font-size", "16px")
			.text(`Title: ${title_text}`));

	// --------------------

}

// -----------------------------------------------






// -------------------------------------------------
// IMAGE SUBFUNCTIONS
// -------------------------------------------------
async function create_dynamic_canvasElement(w) {

  	const index = 0;
  
	// Create a canvas element
	var canvasElement = document.createElement('canvas');

	// Set the width and height of the canvas
	canvasElement.width = w;
	canvasElement.height = canvasElement.width;
	      
	// Get the 2D rendering context of the canvas
	var ctx = canvasElement.getContext("2d");
	
	if (index == 0) {
		canvasElement.style.left = 40+'px';
	} else {
		let tot = index*canvasElement.width + 40;
		canvasElement.style.left = tot+'px';
	}
	
	// Add the canvas to the document body or any other desired element
	document.getElementById('data_display').appendChild(canvasElement);

	return ctx;
}

// -------------------------------------------------

	

	


// -------------------------------------------------
// AUDIO SUBFUNCTIONS
// -------------------------------------------------
async function play_sound_from_a_blob_object(blob_object, file_type) {

	// ie: file_type = "audio/mp3"
	
	// Putting an array, or mediastream event.data in a blob resulted in a Security Error
	var file_blob_object = new File ([blob_object], 'recorded_audio', {type: file_type});
	var url_file_blob_object = URL.createObjectURL(file_blob_object);
	
	await load_audio_from_url(url_file_blob_object, mode="nocors");
	
	URL.revokeObjectURL(url_file_blob_object);
  
}
  
// -------------------------------------------------

async function load_audio_from_url(url, mode="nocors") {

	// Create an AudioElement
	const audioElement = document.createElement('audio');
	
	audioElement.src = url;
	audioElement.id = "audio track";
	audioElement.autoplay = true;
	audioElement.setAttribute("controls", true);
	audioElement.setAttribute('preload', "auto");

	if (mode == "cors") {
		audioElement.setAttribute('crossOrigin', "anonymous");
	}

  	// Create an SourceElement (audioSourceNode)
	// var sourceElement = document.createElement('source');
	// sourceElement.setAttribute("src", url);
	// console.log('sourceElement: ', sourceElement);
  
	// audioElement.appendChild(sourceElement);
	
	document.getElementById('data_display').appendChild(audioElement);

	return audioElement;
}

// -------------------------------------------------

async function evaluate_audioElement() {

	// It appears that if the audioElement is in memory, these functions can be used

	// -----------------------------------------------
	
	// Create an AudioContext
	// var audioContext = new (window.AudioContext || window.webkitAudioContext)();
	// audioContext = new AudioContext();
	// OR
	var audioContext = new AudioContext();
 
	// -----------------------------------------------

	// Create an AnalyserNode: some signal information was more reliable with analyserNode than audioContext
	var analyserNode = audioContext.createAnalyser();
	// console.log('analyserNode: ', analyserNode);
	
	var channels = audioContext.channelCount;
	if (channels == undefined) { 
		channels = analyserNode.channelCount;
	}
	// console.log('channels: ', channels);
	
	var fs = audioContext.sampleRate;
	if (fs == undefined) { 
		fs = analyserNode.context.sampleRate; // 48000
	}
	// console.log('fs: ', fs);

	let time_length = analyserNode.context.currentTime;   // 354.976
	// console.log('time_length: ', time_length);

	// -----------------------------------------------

	const frameCount = fs * 2.0;
	
	var time_sample_rate_OR_timePeriod = (1/fs) * 1000; // every time_sample_rate there is a data point, 0.020833333333333333
	// console.log('time_sample_rate_OR_timePeriod: ', time_sample_rate_OR_timePeriod);

	// -----------------------------------------------
	
	return {channels: channels, fs: fs, time_length: time_length, frameCount: frameCount, time_sample_rate_OR_timePeriod: time_sample_rate_OR_timePeriod};
}

// -------------------------------------------------
  
  
  


// -------------------------------------------------
// AUDIO PRE-PROCESSING SUBFUNCTIONS
// -------------------------------------------------
async function decodeAudioData_from_binaryString_to_uint8Array(binaryString) {
	
	// -----------------------------------------------
	// Decode the binaryString response
	// -----------------------------------------------
	var character_array = binaryString.split('');
	console.log("character_array: ", character_array);
	// Array(38190) [ "�", "�", "�", "d", "\u0000", "\u0000", "\u0000", "\u0000", "\u0000", "\u0000", … ]
			
	// Map each [binary string character; a subset of binary string characters is UTF-8] as an [ASCII number; a number from 0 to number_of_characters]
	var byteArray = character_array.map((character) => { return character.charCodeAt(0); });
	console.log("byteArray: ", byteArray);
	// byteArray:  Array(38190) [ 65533, 65533, 65533, 100, 0, 0, 0, 0, 0, 0, … ]

	// The importance of this mapping is to limit the array values from 0 to 255.
	var uint8Array = new Uint8Array(byteArray);
	console.log('uint8Array: ', uint8Array);
	// uint8Array:  Uint8Array(38190) [ 253, 253, 253, 100, 0, 0, 0, 0, 0, 0, … ]

	// In some ways a uint8Array is an arrayBuffer because the size is "fixed" meaning that no more data will be appended to the array after the UTF-8 characters. And, it is a "fixed" array because the values of the array are limited to a certain range of numbers, from 0 to 255. 

	// Convert UTF-8 array [non-fixed length array] to a binary arrayBuffer [fixed-length array]
	const arrayBuffer = uint8Array.buffer;
	console.log('arrayBuffer: ', arrayBuffer);

	// Determine the length of the typedArray_arrayBuffer
	console.log('arrayBuffer.byteLength: ', arrayBuffer.byteLength);
	
	// --------------------
	
	// Convert the TypedArray into a normal array
	const normalArray = await Array.from(uint8Array);
	console.log('normalArray: ', normalArray);

	// Determine the length of the normalArray.length
	console.log('normalArray.length: ', normalArray.length);

	var arr_char = await obtain_array_characteristics(normalArray);
	console.log('arr_char: ', arr_char);

	// --------------------

	// Normalize the audio data in arrayBuffer from [-1, 1]
	// var normalArray_normalized = await normalArray.map((val, ind) => { return val/arr_char.max_amp; });  // gave wrong results
	// var normalArray_normalized = await normalArray.map((val, ind) => { return val/255; }); 
	// console.log('normalArray max min: ', [normalArray_normalized.sort().shift(), normalArray_normalized.sort().pop()])
	// RESULT: it gives a value from 0 to 2.5656... not exactly what was expected
	
	// OR

	// Try with Tensorflow library
	const tf_normalArray = tf.tensor1d(normalArray);
	var normalArray_normalized_tf = await tf_normalArray.div(tf.scalar(255));
	  
	// --------------------
	
	// Convert Tensorflow_array to Array
	const normalArray_normalized = normalArray_normalized_tf.arraySync();
	console.log('normalArray max min: ', [normalArray_normalized.sort().shift(), normalArray_normalized.sort().pop()])
	  
	// --------------------
	
	// Only for verification
	var arr_char_normalized = await obtain_array_characteristics(normalArray_normalized);
	console.log('arr_char_normalized: ', arr_char_normalized);
	
	// --------------------

	return normalArray_normalized;
}

// -----------------------------------------------

async function separate_stero_channels(normalArray_normalized) {

  // Audio files with two channels are interleaved, meaning that [channel0_datapoint0, channel1_datapoint0, channel0_datapoint1, channel1_datapoint1, ...channel0_datapointn-1, channel1_datapointn-1] 
  var channel1 = [];
	var channel2 = normalArray_normalized.map((val, ind) => {
		if (ind % 2 == 0) {
			channel1.push(val);
			return '';
		} else {
			return val;
		}
	});
	const NonEmptyVals_toKeep = (x) => x.length != 0;
	channel2 = channel2.filter(NonEmptyVals_toKeep);
  
  return [channel1, channel2];
}

// -----------------------------------------------
  
async function obtain_array_characteristics(arr) {

	var arr_char = {};
	
	arr_char.mu = await mean(arr);
	arr_char.sigma = await std(arr);

	const arr_sort = arr.sort();
	arr_char.max_val = arr_sort.at(arr.length-1);
	arr_char.min_val = arr_sort.at(0);

	// Maximum amplitude
	arr_char.max_amp = [Math.abs(arr_char.min_val), arr_char.max_val].sort().pop();
	
	return arr_char;
}
	
// -----------------------------------------------

async function sum(arr) {
	return arr.reduce((accumulator, currentValue) => accumulator + currentValue, 0);
}

// -----------------------------------------------
  
async function mean(arr) {
	return await sum(arr)/arr.length;
}

// -----------------------------------------------

async function std(arr) {
	const mu =  await mean(arr);
	// console.log('mu: ', mu);

	var arr1 = arr.map((x) => { return x-mu; });
	const summ = await sum(arr1);
	// console.log('summ: ', summ);

	const out = Math.sqrt( Math.pow(summ, 2)/arr.length );
	// console.log('out: ', out);
	
	return out;
}

// -------------------------------------------------

async function tensorflow_FFT(x) {
	
	// Discrete Fourier Transform:  1/sqrt(N) * sum_{0}^{N-1}(x(n) exp^{-j * omega * n}), where omega = 2 * pi * k/N for k=0...N-1
	const N = x.length;
	
	var real = [];
	var imag_plus = [];
	var imag_minus = [];
	
	for (var k=0; k<N; k++) {
		var real_accumulated = 0;
		var imag_plus_accumulated = 0;
		var imag_minus_accumulated = 0;
		
		for (var n=0; n<N; n++) {
			
			var omega = 2 * Math.PI * k/N;
			var theta = omega * n;
			
			// Break the equation down into real and imaginary parts, and sum directly instead of a vector sum at the end of the function
			real_accumulated = real_accumulated + (x.at(n) * -Math.cos(theta));
			imag_plus_accumulated = imag_plus_accumulated + (x.at(n) * Math.sin(theta));
			imag_minus_accumulated = imag_minus_accumulated + (x.at(n) * -Math.sin(theta));
		}
		
		// Save the [accumulated sum of real and imaginary values across n] for each k
		real.push(real_accumulated);
		imag_plus.push(imag_plus_accumulated);
		imag_minus.push(imag_minus_accumulated);
	}

	// Call the tensorflow function to compute the sum of real and imaginary values normalized: (1/Math.sqrt(N) * (real + imag_minus))
	const real_tf = tf.tensor1d(real);
	const imag_tf = tf.tensor1d(imag_minus); // it was suggested that imag_minus should be used instead of imag_plus, but both are correct
	const x_tf = tf.cast(tf.complex(real_tf, imag_tf), 'complex64');
	
	return tf.spectral.fft(x_tf);
}

// -------------------------------------------------


// -------------------------------------------------
  
</script>

</body>
</html>
